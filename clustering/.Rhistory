knitr::opts_chunk$set(echo = TRUE)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
knitr::opts_chunk$set(echo = TRUE)
# using the iris data set explain the k mean clustering
head(iris)
# putting the iris data into ir for doing experiment
# in the ir last colum making null cos we make the cluster based on other feature on the ir data set
ir<-iris
ir$Species<-NULL
head(ir)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# a tabluer comparison of species and cluster made by R
# versicolor 48 in one cluster but 2 in cluster one so thats a mistake
# same as verginica 14 mistake
# this can be treated as a accuricy table of clustering , since clustring is unsupervised learning technique it dont have a target variable to validate the model. perform K-means clustering using R. Clustering is a unsupervised learning algorithm used when the data is unlabeled in nature. k mean clustering is very sensitive in outlier that mean some data point is be missclassified, so if we can remove the outlier its performance can be good.
table(iris$Species,res$cluster)
# with the class plot we can visualise the data in two dimansional space where even the data has more than two component or the variables
library(cluster)
clusplot(iris, res$cluster, color=TRUE, shade=TRUE, labels=0,lines=0 )
# apply c-means with k=3
set.seed(1)
cm <- cmeans(iris, centers=3)
library(limma)
library(gplots)
library(e1071)
library(shiny)
# apply c-means with k=3
set.seed(1)
cm <- cmeans(iris, centers=3)
# set up plots
par(mfrow=c(2,2))
# apply c-means with k=1
set.seed(1)
cm.out2 <- cmeans(data.mat[,-3], centers=2, m = 2)
dim(iris)
head(iris)
table(iris$Species)
class(iris)
# set up plots
par(mfrow=c(2,2))
# plot original data
speciesColors = sapply(as.character(iris$Species), switch, setosa = "red", versicolor = "blue",  virginica="green")
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
colnames(data.mat) <- c("Petal.Length", "Sepal.Width", "Species")
plot(data.mat[,-3], col=speciesColors, main="orignal data with class information")
# apply k-means with k=2
set.seed(1)
km.out2 <- kmeans(data.mat[,-3], centers=2)
plot(data.mat[,-3], col=(km.out2$cluster+1), main="k-means clustering results with k=2", xlab="f1", ylab="f2", pch=20, cex=2)
# apply k-means with k=3
set.seed(1)
km.out3 <- kmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(km.out3$cluster+1), main="k-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=2)
# apply k-means with k=4
set.seed(1)
km.out4 <- kmeans(data.mat[,-3], centers=4)
plot(data.mat[,-3], col=(km.out4$cluster+1), main="k-means clustering results with k=4", xlab="f1", ylab="f2", pch=20, cex=2)
# set up plots
par(mfrow=c(2,2))
# apply c-means with k=1
set.seed(1)
cm.out2 <- cmeans(data.mat[,-3], centers=2, m = 2)
plot(data.mat[,-3], col=(cm.out2$cluster+1), main="c-means clustering results with k=2", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=3
set.seed(1)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=4
set.seed(1)
cm.out4 <- cmeans(data.mat[,-3], centers=4)
plot(data.mat[,-3], col=(cm.out4$cluster+1), main="c-means clustering results with k=4", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=3
set.seed(1)
cm <- cmeans(iris, centers=3)
# apply c-means with k=3
set.seed(1)
cm <- cmeans(iris, centers=3, m=2)
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
set.seed(1)
cm <- cmeans(iris, centers=3, m=2)
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
View(data.mat)
View(data.mat)
set.seed(1)
cm.out3 <- cmeans(iris, centers=3)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(data.mat[,-3], nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(data.mat[,-3], nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
knitr::opts_chunk$set(echo = TRUE)
library(limma)
library(gplots)
library(e1071)
library(shiny)
# using the iris data set explain the k mean clustering
head(iris)
# putting the iris data into ir for doing experiment
# in the ir last colum making null cos we make the cluster based on other feature on the ir data set
ir<-iris
ir$Species<-NULL
head(ir)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# a tabluer comparison of species and cluster made by R
# versicolor 48 in one cluster but 2 in cluster one so thats a mistake
# same as verginica 14 mistake
# this can be treated as a accuricy table of clustering , since clustring is unsupervised learning technique it dont have a target variable to validate the model. perform K-means clustering using R. Clustering is a unsupervised learning algorithm used when the data is unlabeled in nature. k mean clustering is very sensitive in outlier that mean some data point is be missclassified, so if we can remove the outlier its performance can be good.
table(iris$Species,res$cluster)
# with the class plot we can visualise the data in two dimansional space where even the data has more than two component or the variables
library(cluster)
clusplot(iris, res$cluster, color=TRUE, shade=TRUE, labels=0,lines=0 )
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", #ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out2 <- cmeans(data.mat[,-3], centers=2, m = 2)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(data.mat[,-3], nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
knitr::opts_chunk$set(echo = TRUE)
library(limma)
library(gplots)
library(e1071)
library(shiny)
# using the iris data set explain the k mean clustering
head(iris)
# putting the iris data into ir for doing experiment
# in the ir last colum making null cos we make the cluster based on other feature on the ir data set
ir<-iris
ir$Species<-NULL
head(ir)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# a tabluer comparison of species and cluster made by R
# versicolor 48 in one cluster but 2 in cluster one so thats a mistake
# same as verginica 14 mistake
# this can be treated as a accuricy table of clustering , since clustring is unsupervised learning technique it dont have a target variable to validate the model. perform K-means clustering using R. Clustering is a unsupervised learning algorithm used when the data is unlabeled in nature. k mean clustering is very sensitive in outlier that mean some data point is be missclassified, so if we can remove the outlier its performance can be good.
table(iris$Species,res$cluster)
# with the class plot we can visualise the data in two dimansional space where even the data has more than two component or the variables
library(cluster)
clusplot(iris, res$cluster, color=TRUE, shade=TRUE, labels=0,lines=0 )
# apply c-means with k=3
data.mat <- cbind(iris$Petal.Length, iris$Sepal.Width, iris$Species)
# apply c-means with k=3
set.seed(1)
cm.out2 <- cmeans(data.mat[,-3], centers=2, m = 2)
cm.out3 <- cmeans(data.mat[,-3], centers=3)
plot(data.mat[,-3], col=(cm.out3$cluster+1), main="c-means clustering results with k=3", xlab="f1", ylab="f2", pch=20, cex=apply(cm.out2$membership, 1, max)^2)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(data.mat[,-3], nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
cm1<-cmeans (ir, centers, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
cm1<-cmeans (ir, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
cm1<-cmeans (ir, centers=2, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(ir, nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
View(cm1)
View(cm1)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=cm1$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
cm1<-cmeans (ir, centers=3, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=cm1$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
knitr::opts_chunk$set(echo = TRUE)
library(limma)
library(gplots)
library(e1071)
library(shiny)
# using the iris data set explain the k mean clustering
head(iris)
# putting the iris data into ir for doing experiment
# in the ir last colum making null cos we make the cluster based on other feature on the ir data set
ir<-iris
ir$Species<-NULL
head(ir)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# a tabluer comparison of species and cluster made by R
# versicolor 48 in one cluster but 2 in cluster one so thats a mistake
# same as verginica 14 mistake
# this can be treated as a accuricy table of clustering , since clustring is unsupervised learning technique it dont have a target variable to validate the model. perform K-means clustering using R. Clustering is a unsupervised learning algorithm used when the data is unlabeled in nature. k mean clustering is very sensitive in outlier that mean some data point is be missclassified, so if we can remove the outlier its performance can be good.
table(iris$Species,res$cluster)
# with the class plot we can visualise the data in two dimansional space where even the data has more than two component or the variables
library(cluster)
clusplot(iris, res$cluster, color=TRUE, shade=TRUE, labels=0,lines=0 )
# c mean clustering using 3 cluster
cm1<-cmeans (ir, centers=3, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=cm1$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(ir, nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
knitr::opts_chunk$set(echo = TRUE)
library(limma)
library(gplots)
library(e1071)
library(shiny)
# using the iris data set explain the k mean clustering
head(iris)
# putting the iris data into ir for doing experiment
# in the ir last colum making null cos we make the cluster based on other feature on the ir data set
ir<-iris
ir$Species<-NULL
head(ir)
# based on all the feature kmeans clustering algorithm been applying
# the description is below
# 3 cluster
res<-kmeans(ir,3)
res
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=res$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
# a tabluer comparison of species and cluster made by R
# versicolor 48 in one cluster but 2 in cluster one so thats a mistake
# same as verginica 14 mistake
# this can be treated as a accuricy table of clustering , since clustring is unsupervised learning technique it dont have a target variable to validate the model. perform K-means clustering using R. Clustering is a unsupervised learning algorithm used when the data is unlabeled in nature. k mean clustering is very sensitive in outlier that mean some data point is be missclassified, so if we can remove the outlier its performance can be good.
table(iris$Species,res$cluster)
# with the class plot we can visualise the data in two dimansional space where even the data has more than two component or the variables
library(cluster)
clusplot(iris, res$cluster, color=TRUE, shade=TRUE, labels=0,lines=0 )
# c mean clustering using 3 cluster
cm1<-cmeans (ir, centers=3, iter.max=100, verbose=FALSE, dist="euclidean",
method="cmeans", m=2, rate.par = NULL)
# plot them according to the cluster
iris<-iris
plot(iris[c("Petal.Length", "Petal.Width")], col=cm1$cluster)# this is based on our made cluster
# how is the cluster in the original data set
plot(iris[c("Petal.Length", "Petal.Width")], col=iris$Species)# so both of the fig almost same, so we did a good job though
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(ir, nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
#install.packages("clvalid")
library(clValid) # clValid is a package that containing various cluster validation methods
intern <- clValid(ir, nClust=2:10, validation="internal", clMethods=c("hierarchical","kmeans", "pam"))
summary(intern)
optimalScores(intern)
# set up plots
par(mfrow=c(2,2))
plot(intern)
table(iris$Species,cm1$cluster)
knitr::opts_chunk$set(echo = TRUE)
# Usually when dealing with an unsupervised learning problem, its difficult to get a good measure of how well the model performed. For this project, we will use data from the UCI archive based off of red and white wines (this is a very commonly used data set in ML).
redwine = read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", header= TRUE, sep = ";", dec = ".")
whitewine=read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv", header= TRUE, sep = ";", dec = ".")
# ADDING EXTRA LEBEL IN THE END OF THE FILE WHICH WILL USE USE FOR FUTURE EVALUATION
redwine[,13]<-'red'
whitewine[,13]<-'white'
#Combine both data file into a single data frame called wine.
wine <- rbind(redwine,whitewine)
str(wine)
# APPLYING THE K MEAN CLUSTERING ALGORITHM WITHOUT INFORMING THE ALGORITHM THE NAME OF THE LABEL
# WE USING TWO CLUSTER HERE
wine.cluster <- kmeans(wine[1:12],2)
wine.cluster
wine.cluster$centers
# now lets see how the algorithm is working while known lebel was comparing
# red wine classified well
table(wine$V13,wine.cluster$cluster)
wine.cluster <- kmeans(wine[1:12],2)
wine.cluster
wine.cluster <- cmeans(wine[1:12],2)
wine.cluster
wine.cm <- cmeans(wine[1:12],2)
wine.cm
wine.cm <- cmeans(wine[1:12],2)
#wine.cm
table(wine$V13,wine.cm$cluster)
wine.cm <- cmeans(wine[1:12],2)
#wine.cm
table(wine$V13,wine.cm$cluster)
library(limma)
library(gplots)
library(e1071)
library(shiny)
